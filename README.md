# Activation Functions in Neural Networks

In this repository, youâ€™ll find code implementations of several activation functions commonly used in neural networks. Activation functions play a crucial role in transforming inputs into outputs in each layer, introducing non-linearity to help the model learn complex patterns and relationships. 

### Activation Functions Implemented
- **Sigmoid**: Smooth activation that outputs values between 0 and 1. [Code](./Activation%20Functions/sigmoid_activation.ipynb).
- **ReLU (Rectified Linear Unit)**: Sets negative values to 0, often used for hidden layers. [Code](./Activation%20Functions/relu_activation.ipynb)
- **Softmax**: Commonly used for multi-class classification, converting outputs to probabilities. [Code](./Activation%20Functions/softmax_function.ipynb)

### Application Example
- **Multi-Class Classification**: Using Softmax and other activation functions, the implementation demonstrates how neural networks can handle classification tasks across multiple categories effectively. [Code](./Activation%20Functions/multi_class_classification.ipynb)

Each function includes both an explanation and code implementation to demonstrate its role and usage in neural networks.
