# Activation Functions in Neural Networks

In this repository, youâ€™ll find code implementations of several activation functions commonly used in neural networks. Activation functions play a crucial role in transforming inputs into outputs in each layer, introducing non-linearity to help the model learn complex patterns and relationships. 

### Activation Functions Implemented
- **Sigmoid**: Smooth activation that outputs values between 0 and 1.
- **ReLU (Rectified Linear Unit)**: Sets negative values to 0, often used for hidden layers.
- **Softmax**: Commonly used for multi-class classification, converting outputs to probabilities.

### Application Example
- **Multi-Class Classification**: Using Softmax and other activation functions, the implementation demonstrates how neural networks can handle classification tasks across multiple categories effectively.

Each function includes both an explanation and code implementation to demonstrate its role and usage in neural networks.
