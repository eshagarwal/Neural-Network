{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification\n",
    "\n",
    "Multi-class classification is a type of supervised machine learning problem where the goal is to categorize input data into one of three or more classes or categories. Unlike binary classification, which deals with only two possible outcomes, multi-class classification involves multiple classes that are mutually exclusive.\n",
    "\n",
    "### Key Characteristics of Multi-Class Classification:\n",
    "- **Multiple Classes**: The model predicts one class label from a set of multiple possible classes.\n",
    "- **Single Output**: For each input instance, only one class label can be assigned, meaning the classes are typically exclusive (e.g., an email can be classified as either \"spam,\" \"promotional,\" or \"important,\" but not more than one at a time).\n",
    "- **Common Applications**: Multi-class classification is widely used in various applications, including:\n",
    "  - Image recognition (e.g., classifying objects in images)\n",
    "  - Document classification (e.g., categorizing news articles)\n",
    "  - Sentiment analysis (e.g., classifying reviews as positive, negative, or neutral)\n",
    "\n",
    "### Evaluation Metrics:\n",
    "To evaluate the performance of multi-class classification models, several metrics can be used, including:\n",
    "- **Accuracy**: The proportion of correctly predicted instances over the total number of instances.\n",
    "- **Confusion Matrix**: A table that summarizes the performance of the model by comparing predicted and actual class labels.\n",
    "- **F1 Score**: The harmonic mean of precision and recall, useful for understanding the balance between false positives and false negatives.\n",
    "\n",
    "In multi-class classification tasks, models often use techniques like Softmax activation in the output layer to predict probabilities for each class, allowing the selection of the most likely class based on the computed scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a 2-Layer Neural Network for Multi-Class Classification\n",
    "\n",
    "Constructed a simple 2-layer neural network using TensorFlow and Keras for multi-class classification. The main components of this implementation are as follows:\n",
    "\n",
    "### 1. Model Definition\n",
    "- We defined a function `create_model` that constructs a neural network with one hidden layer and one output layer. \n",
    "- The hidden layer consists of 128 neurons with ReLU (Rectified Linear Unit) activation, allowing the network to learn non-linear relationships in the data.\n",
    "- The output layer has a linear activation function, producing raw scores (logits) for each class without applying a softmax function. This design choice is made for numerical stability during training.\n",
    "\n",
    "### 2. Input and Output Specifications\n",
    "- The model is designed to accept input samples with 10 features, which is indicated by the input shape `(10,)`.\n",
    "- The output layer is configured to predict probabilities for four classes, making this network suitable for multi-class classification tasks.\n",
    "\n",
    "### 3. Model Compilation\n",
    "- The model is compiled using the Adam optimizer, a popular choice for training neural networks due to its adaptive learning rate properties.\n",
    "- We used `sparse_categorical_crossentropy` as the loss function, which is appropriate for multi-class classification problems where class labels are provided as integers.\n",
    "\n",
    "### 4. Data Generation and Training\n",
    "- Random training data is generated to simulate a dataset for training the model. The dataset consists of 1000 samples, each with 10 features, along with random class labels from 0 to 3.\n",
    "- The model is trained for 10 epochs with a batch size of 32, allowing it to learn from the training data.\n",
    "\n",
    "### 5. Making Predictions\n",
    "- After training, the model is used to make predictions on a new input example. \n",
    "- The linear outputs are processed using the Softmax function to convert them into probabilities, representing the model's confidence in each class.\n",
    "- The predicted class is determined by selecting the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,924</span> (7.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,924\u001b[0m (7.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,924</span> (7.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,924\u001b[0m (7.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),  # Hidden layer\n",
    "        layers.Dense(num_classes)  # Output layer with linear activation\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Set parameters\n",
    "input_shape = (10,)  # Correct shape for 10 features\n",
    "num_classes = 4  # Number of output classes\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',  # Suitable loss for multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - accuracy: 0.2560 - loss: 1.7734\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401us/step - accuracy: 0.2266 - loss: 1.3863\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - accuracy: 0.2347 - loss: 1.3863\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441us/step - accuracy: 0.2558 - loss: 1.3863\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2136 - loss: 1.3863\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435us/step - accuracy: 0.2309 - loss: 1.3863\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - accuracy: 0.2397 - loss: 1.3863\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - accuracy: 0.2484 - loss: 1.3863\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - accuracy: 0.2726 - loss: 1.3863\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - accuracy: 0.2379 - loss: 1.3863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Linear Output: [[-0.47821915 -0.40279412 -0.484405   -0.50008506]]\n",
      "Softmax Probabilities: [[0.24688022 0.2662214  0.24535777 0.24154055]]\n",
      "Predicted Class: [1]\n"
     ]
    }
   ],
   "source": [
    "# Example: Generate some random data for training\n",
    "\n",
    "# Random training data (1000 samples, 10 features)\n",
    "X_train = np.random.random((1000, 10))\n",
    "y_train = np.random.randint(num_classes, size=(1000,))  # Random class labels\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Example of making predictions\n",
    "example_input = np.random.random((1, 10))  # A new input example\n",
    "linear_output = model.predict(example_input)  # Get linear outputs\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = tf.nn.softmax(linear_output)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = tf.argmax(probabilities, axis=1)\n",
    "\n",
    "print(\"Linear Output:\", linear_output)\n",
    "print(\"Softmax Probabilities:\", probabilities.numpy())\n",
    "print(\"Predicted Class:\", predicted_class.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
